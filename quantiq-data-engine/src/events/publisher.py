"""
Event Publisher for Quantiq Data Engine

Kafkaë¡œ ì´ë²¤íŠ¸ë¥¼ ë°œí–‰í•˜ëŠ” ë²”ìš© ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
"""

from confluent_kafka import Producer
import json
import logging
from src.core.config import settings
from src.events.schema import BaseEvent

logger = logging.getLogger(__name__)


class EventPublisher:
    """
    Kafka Event Publisher (Singleton Pattern)
    """
    _producer = None

    @classmethod
    def get_producer(cls):
        """Kafka Producer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (Singleton)"""
        if cls._producer is None:
            try:
                conf = {
                    'bootstrap.servers': settings.KAFKA_BOOTSTRAP_SERVERS,
                    'client.id': 'quantiq-data-engine',
                    # Reliability settings
                    'acks': 'all',  # ëª¨ë“  replica í™•ì¸
                    'retries': 3,  # ì¬ì‹œë„ 3íšŒ
                    'retry.backoff.ms': 100,  # ì¬ì‹œë„ ê°„ê²© 100ms
                    # Performance settings
                    'batch.size': 16384,  # 16KB
                    'linger.ms': 10,  # 10ms ëŒ€ê¸° í›„ ë°°ì¹˜ ì „ì†¡
                    'compression.type': 'snappy',  # ì••ì¶•
                    # Idempotence for exactly-once semantics
                    'enable.idempotence': True
                }
                cls._producer = Producer(conf)
                logger.info(f"ğŸ“¡ Kafka producer created for {settings.KAFKA_BOOTSTRAP_SERVERS}")
            except Exception as e:
                logger.error(f"âŒ Failed to create Kafka producer: {e}")
                raise
        return cls._producer

    @classmethod
    def delivery_report(cls, err, msg):
        """ë©”ì‹œì§€ ì „ì†¡ ê²°ê³¼ ì½œë°±"""
        if err is not None:
            logger.error(f'âŒ Message delivery failed: {err}')
        else:
            logger.info(f'âœ… Message delivered to {msg.topic()} [{msg.partition()}]')

    @classmethod
    def publish(cls, topic: str, event: BaseEvent):
        """
        ì´ë²¤íŠ¸ë¥¼ Kafka í† í”½ì— ë°œí–‰í•©ë‹ˆë‹¤

        Args:
            topic: Kafka í† í”½ëª…
            event: ë°œí–‰í•  ì´ë²¤íŠ¸ (BaseEvent)
        """
        try:
            p = cls.get_producer()
            message = json.dumps(event.to_dict()).encode('utf-8')

            logger.info(f"ğŸ“¤ Publishing event to topic [{topic}]: eventId={event.eventId}, type={event.eventType}")
            logger.debug(f"Event payload: {message}")

            # Asynchronous produce
            p.produce(
                topic,
                message,
                callback=cls.delivery_report
            )

            # Flush to ensure delivery
            # In high throughput scenarios, this should be managed differently
            p.flush()

        except Exception as e:
            logger.error(f"âŒ Failed to publish event to {topic}: {e}")
            import traceback
            logger.error(traceback.format_exc())

    @classmethod
    def close(cls):
        """Producerë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤"""
        if cls._producer is not None:
            cls._producer.flush()
            logger.info("ğŸ“¡ Kafka producer closed")


# ============================================================================
# Legacy Support
# ============================================================================

def publish_event(event_type: str, payload: dict):
    """
    Legacy í˜¸í™˜ì„±ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜

    Deprecated: create_event()ì™€ EventPublisher.publish()ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì„¸ìš”
    """
    from src.events.schema import create_event

    logger.warning("âš ï¸ publish_event() is deprecated. Use EventPublisher.publish() instead.")

    # Legacy event formatì„ BaseEventë¡œ ë³€í™˜
    event = BaseEvent(
        eventType=event_type,
        payload=payload
    )

    EventPublisher.publish(settings.KAFKA_TOPIC_ANALYSIS_COMPLETED, event)
