"""
Quantiq Data Engine - Message Processing Worker

Architecture:
- Primary: Kafka message processing (all data operations)
- Secondary: Read-only REST API (health checks, status queries)
"""
import logging
import json
import time
import threading
import subprocess
import sys
from pathlib import Path
from fastapi import FastAPI
import uvicorn
from confluent_kafka import Consumer, KafkaError
from datetime import datetime
from pytz import timezone

from src.core.config import settings
from src.core.database import MongoDB
from src.features.economic_data.router import router as economic_router
from src.features.ml_package.router import router as ml_package_router
from src.features.economic_data.service import EconomicDataService
from src.services.recommendation_service import RecommendationService
from src.services.slack_notifier import SlackNotifier
from src.core.kafka import KafkaEventPublisher

KST = timezone('Asia/Seoul')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# FastAPI app (Read-Only Status API)
app = FastAPI(
    title="Quantiq Data Engine",
    description="Message Processing Worker with Read-Only Status API"
)

# Include routers (status endpoints only)
app.include_router(economic_router)
app.include_router(ml_package_router)


@app.get("/")
def read_root():
    return {
        "service": "Quantiq Data Engine",
        "architecture": "Message Processing Worker",
        "status": "running",
        "subscribed_kafka_topics": [
            "economic.data.update.request",
            "analysis.technical.request",
            "analysis.sentiment.request",
            "analysis.combined.request",
            "ml.package.upload.request"
        ],
        "api_purpose": "Read-only health checks and status queries",
        "timestamp": datetime.now(KST).isoformat()
    }


@app.get("/health")
def health_check():
    return {"status": "alive", "timestamp": datetime.now(KST).isoformat()}


def run_api():
    logger.info("Starting Data Engine API server on port 8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)


def main():
    logger.info("Quantiq Data Engine Started (Feature-based Architecture)")

    # Start API server in a separate thread
    api_thread = threading.Thread(target=run_api, daemon=True)
    api_thread.start()

    # 1. Start MongoDB connection
    db = MongoDB.get_db()
    if db is None:
        logger.error("Failed to connect to MongoDB")
        return

    # 2. Setup Kafka Consumer
    conf = {
        'bootstrap.servers': settings.KAFKA_BOOTSTRAP_SERVERS,
        'group.id': 'quantiq-data-engine-fresh',  # Fresh consumer group for clean start
        'auto.offset.reset': 'earliest'
    }

    # Wait for Kafka to be ready
    time.sleep(10)

    consumer = Consumer(conf)

    # í† í”½ êµ¬ë… (ê²½ì œ ë°ì´í„° + ë¶„ì„ ìš”ì²­ + ML íŒ¨í‚¤ì§€)
    topics = [
        settings.KAFKA_TOPIC_ECONOMIC_DATA_UPDATE_REQUEST,
        "analysis.technical.request",
        "analysis.sentiment.request",
        "analysis.combined.request",
        "ml.package.upload.request"
    ]
    consumer.subscribe(topics)
    logger.info(f"Subscribed to topics: {topics}")

    # Services ì´ˆê¸°í™”
    economic_service = EconomicDataService()
    recommendation_service = RecommendationService()

    try:
        while True:
            msg = consumer.poll(1.0)

            if msg is None:
                continue
            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    continue
                else:
                    logger.error(f"Consumer error: {msg.error()}")
                    continue

            try:
                topic_name = msg.topic()
                message = json.loads(msg.value().decode('utf-8'))
                logger.info(f"Received request from topic '{topic_name}': {message}")

                # ê²½ì œ ë°ì´í„° ì—…ë°ì´íŠ¸ ìš”ì²­ ì²˜ë¦¬
                if topic_name == settings.KAFKA_TOPIC_ECONOMIC_DATA_UPDATE_REQUEST:
                    # payload í•„ë“œì—ì„œ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ
                    payload = message.get("payload", message)
                    request_id = payload.get("requestId", "unknown")
                    source = payload.get("source", "kafka")
                    thread_ts = payload.get("threadTs")  # Kotlinì—ì„œ ì „ë‹¬ë°›ì€ ìŠ¤ë ˆë“œ íƒ€ì„ìŠ¤íƒ¬í”„
                    target_date = payload.get("targetDate")  # ìˆ˜ì§‘í•  ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD)

                    logger.info("=" * 80)
                    logger.info("ê²½ì œ ë°ì´í„° ì—…ë°ì´íŠ¸ Kafka ë©”ì‹œì§€ ìˆ˜ì‹ ")
                    logger.info(f"Request ID: {request_id}")
                    logger.info(f"Target Date: {target_date or 'ë‹¹ì¼'}")
                    logger.info(f"Thread TS: {thread_ts}")
                    logger.info("=" * 80)

                    # ğŸ”” ìˆ˜ì§‘ ì‹œì‘ ì•Œë¦¼ (ìŠ¤ë ˆë“œ ë‹µê¸€)
                    SlackNotifier.notify_economic_data_collection_start(request_id, source, thread_ts)

                    start_time = time.time()
                    try:
                        # Service í˜¸ì¶œ (ë‚ ì§œ íŒŒë¼ë¯¸í„° ì „ë‹¬)
                        result = economic_service.collect_economic_data(target_date=target_date)
                        elapsed_time = time.time() - start_time

                        logger.info("âœ… ê²½ì œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")

                        # ìˆ˜ì§‘ ê²°ê³¼ ë°ì´í„° êµ¬ì„±
                        collection_summary = {
                            "target_date": result.get("target_date"),
                            "duration": f"{elapsed_time:.2f}ì´ˆ",
                            "fred_collected": result.get("fred_collected", 0),
                            "yahoo_collected": result.get("yahoo_collected", 0),
                            "total_indicators": result.get("fred_collected", 0) + result.get("yahoo_collected", 0)
                        }

                        # ğŸ”” ìˆ˜ì§‘ ì™„ë£Œ ì•Œë¦¼ (ìŠ¤ë ˆë“œ ë‹µê¸€)
                        SlackNotifier.notify_economic_data_collection_success(
                            request_id,
                            collection_summary,
                            thread_ts
                        )

                        # ì™„ë£Œ ì´ë²¤íŠ¸ ë°œí–‰
                        KafkaEventPublisher.publish("ECONOMIC_DATA_UPDATED", {
                            "status": "success",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "duration": elapsed_time
                        })
                    except Exception as e:
                        logger.error(f"âŒ ê²½ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

                        # ğŸ”” ì˜¤ë¥˜ ì•Œë¦¼ (ìŠ¤ë ˆë“œ ë‹µê¸€)
                        SlackNotifier.notify_economic_data_collection_error(request_id, str(e), thread_ts)

                        # ì˜¤ë¥˜ ì´ë²¤íŠ¸ ë°œí–‰
                        KafkaEventPublisher.publish("ECONOMIC_DATA_UPDATE_FAILED", {
                            "status": "failed",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "error": str(e)
                        })
                        raise

                # ê¸°ìˆ ì  ë¶„ì„ ìš”ì²­ ì²˜ë¦¬
                elif topic_name == "analysis.technical.request":
                    payload = message.get("payload", message)
                    request_id = payload.get("requestId", "unknown")
                    thread_ts = payload.get("threadTs")  # Kotlinì—ì„œ ì „ë‹¬ë°›ì€ ìŠ¤ë ˆë“œ íƒ€ì„ìŠ¤íƒ¬í”„

                    logger.info("=" * 80)
                    logger.info("ê¸°ìˆ ì  ë¶„ì„ ìš”ì²­ Kafka ë©”ì‹œì§€ ìˆ˜ì‹ ")
                    logger.info(f"Request ID: {request_id}")
                    logger.info(f"Thread TS: {thread_ts}")
                    logger.info("=" * 80)

                    start_time = time.time()
                    try:
                        # Service í˜¸ì¶œ
                        result = recommendation_service.run_technical_analysis(request_id, thread_ts)
                        elapsed_time = time.time() - start_time

                        logger.info("âœ… ê¸°ìˆ ì  ë¶„ì„ ì™„ë£Œ")

                        # ì™„ë£Œ ì´ë²¤íŠ¸ ë°œí–‰
                        KafkaEventPublisher.publish("ANALYSIS_TECHNICAL_COMPLETED", {
                            "status": "success",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "duration": elapsed_time,
                            "result": result
                        })
                    except Exception as e:
                        logger.error(f"âŒ ê¸°ìˆ ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
                        KafkaEventPublisher.publish("ANALYSIS_TECHNICAL_FAILED", {
                            "status": "failed",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "error": str(e)
                        })

                # ê°ì • ë¶„ì„ ìš”ì²­ ì²˜ë¦¬
                elif topic_name == "analysis.sentiment.request":
                    payload = message.get("payload", message)
                    request_id = payload.get("requestId", "unknown")
                    thread_ts = payload.get("threadTs")

                    logger.info("=" * 80)
                    logger.info("ë‰´ìŠ¤ ê°ì • ë¶„ì„ ìš”ì²­ Kafka ë©”ì‹œì§€ ìˆ˜ì‹ ")
                    logger.info(f"Request ID: {request_id}")
                    logger.info(f"Thread TS: {thread_ts}")
                    logger.info("=" * 80)

                    start_time = time.time()
                    try:
                        result = recommendation_service.run_sentiment_analysis(request_id, thread_ts)
                        elapsed_time = time.time() - start_time

                        logger.info("âœ… ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì™„ë£Œ")

                        KafkaEventPublisher.publish("ANALYSIS_SENTIMENT_COMPLETED", {
                            "status": "success",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "duration": elapsed_time,
                            "result": result
                        })
                    except Exception as e:
                        logger.error(f"âŒ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                        KafkaEventPublisher.publish("ANALYSIS_SENTIMENT_FAILED", {
                            "status": "failed",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "error": str(e)
                        })

                # í†µí•© ë¶„ì„ ìš”ì²­ ì²˜ë¦¬
                elif topic_name == "analysis.combined.request":
                    payload = message.get("payload", message)
                    request_id = payload.get("requestId", "unknown")
                    thread_ts = payload.get("threadTs")

                    logger.info("=" * 80)
                    logger.info("í†µí•© ë¶„ì„ ìš”ì²­ Kafka ë©”ì‹œì§€ ìˆ˜ì‹ ")
                    logger.info(f"Request ID: {request_id}")
                    logger.info(f"Thread TS: {thread_ts}")
                    logger.info("=" * 80)

                    start_time = time.time()
                    try:
                        result = recommendation_service.run_combined_analysis(request_id, thread_ts)
                        elapsed_time = time.time() - start_time

                        logger.info("âœ… í†µí•© ë¶„ì„ ì™„ë£Œ")

                        KafkaEventPublisher.publish("ANALYSIS_COMPLETED", {
                            "status": "success",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "duration": elapsed_time,
                            "result": result
                        })
                    except Exception as e:
                        logger.error(f"âŒ í†µí•© ë¶„ì„ ì‹¤íŒ¨: {e}")
                        KafkaEventPublisher.publish("ANALYSIS_FAILED", {
                            "status": "failed",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "error": str(e)
                        })

                # ML íŒ¨í‚¤ì§€ ì—…ë¡œë“œ ìš”ì²­ ì²˜ë¦¬
                elif topic_name == "ml.package.upload.request":
                    payload = message.get("payload", message)
                    request_id = payload.get("requestId", "unknown")
                    thread_ts = payload.get("threadTs")
                    script_path = payload.get("scriptPath", "predict_optimized.py")

                    logger.info("=" * 80)
                    logger.info("ML íŒ¨í‚¤ì§€ ì—…ë¡œë“œ ìš”ì²­ Kafka ë©”ì‹œì§€ ìˆ˜ì‹ ")
                    logger.info(f"Request ID: {request_id}")
                    logger.info(f"Script Path: {script_path}")
                    logger.info(f"Thread TS: {thread_ts}")
                    logger.info("=" * 80)

                    # ğŸ”” ì—…ë¡œë“œ ì‹œì‘ ì•Œë¦¼
                    SlackNotifier.notify_ml_package_upload_start(request_id, thread_ts)

                    start_time = time.time()
                    try:
                        # upload_to_gcs.py ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
                        script_dir = Path(__file__).parent / "scripts" / "utils"
                        upload_script = script_dir / "upload_to_gcs.py"
                        predict_script = script_dir / script_path

                        if not upload_script.exists():
                            raise FileNotFoundError(f"ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {upload_script}")
                        if not predict_script.exists():
                            raise FileNotFoundError(f"ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {predict_script}")

                        logger.info(f"ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸: {upload_script}")
                        logger.info(f"ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸: {predict_script}")

                        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
                        logger.info("GCS ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘...")
                        result = subprocess.run(
                            [sys.executable, str(upload_script), "--file", str(predict_script)],
                            capture_output=True,
                            text=True,
                            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                        )

                        # ë¡œê·¸ ì¶œë ¥
                        if result.stdout:
                            logger.info("=== ìŠ¤í¬ë¦½íŠ¸ ì¶œë ¥ ===")
                            for line in result.stdout.split('\n'):
                                if line.strip():
                                    logger.info(line)

                        if result.stderr:
                            logger.warning("=== ìŠ¤í¬ë¦½íŠ¸ ê²½ê³ /ì˜¤ë¥˜ ===")
                            for line in result.stderr.split('\n'):
                                if line.strip():
                                    logger.warning(line)

                        # ì‹¤í–‰ ê²°ê³¼ í™•ì¸
                        if result.returncode != 0:
                            error_msg = f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨ (exit code: {result.returncode})"
                            if result.stderr:
                                error_msg += f"\n{result.stderr}"
                            raise RuntimeError(error_msg)

                        # ì„±ê³µ ì‘ë‹µì—ì„œ GCS URIì™€ ë²„ì „ ì¶”ì¶œ
                        gcs_uri = None
                        version = None
                        combined_output = result.stdout + "\n" + result.stderr

                        for line in combined_output.split('\n'):
                            if "GCS URI:" in line:
                                gcs_uri = line.split("GCS URI:")[-1].strip()
                            if "íŒ¨í‚¤ì§€ ë²„ì „:" in line or "ìƒˆ ë²„ì „:" in line:
                                try:
                                    version_str = line.split(":")[-1].strip().replace("v", "")
                                    version = int(version_str)
                                except (ValueError, IndexError):
                                    pass

                        elapsed_time = time.time() - start_time
                        logger.info("âœ… ML íŒ¨í‚¤ì§€ ì—…ë¡œë“œ ì™„ë£Œ")

                        # ğŸ”” ì—…ë¡œë“œ ì™„ë£Œ ì•Œë¦¼
                        upload_summary = {
                            "gcs_uri": gcs_uri,
                            "version": f"v{version}" if version else "unknown",
                            "duration": f"{elapsed_time:.2f}ì´ˆ"
                        }
                        SlackNotifier.notify_ml_package_upload_success(request_id, upload_summary, thread_ts)

                        # ì™„ë£Œ ì´ë²¤íŠ¸ ë°œí–‰
                        KafkaEventPublisher.publish("ML_PACKAGE_UPLOADED", {
                            "status": "success",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "duration": elapsed_time,
                            "gcsUri": gcs_uri,
                            "version": version
                        })

                    except subprocess.TimeoutExpired:
                        error_msg = "ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ (5ë¶„ ì´ˆê³¼)"
                        logger.error(f"âŒ {error_msg}")
                        SlackNotifier.notify_ml_package_upload_error(request_id, error_msg, thread_ts)
                        KafkaEventPublisher.publish("ML_PACKAGE_UPLOAD_FAILED", {
                            "status": "failed",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "error": error_msg
                        })
                    except Exception as e:
                        logger.error(f"âŒ ML íŒ¨í‚¤ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                        SlackNotifier.notify_ml_package_upload_error(request_id, str(e), thread_ts)
                        KafkaEventPublisher.publish("ML_PACKAGE_UPLOAD_FAILED", {
                            "status": "failed",
                            "timestamp": datetime.now(KST).isoformat(),
                            "requestId": request_id,
                            "error": str(e)
                        })

            except Exception as e:
                logger.error(f"Error processing message: {e}")

    except KeyboardInterrupt:
        pass
    finally:
        consumer.close()


if __name__ == "__main__":
    main()
